{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 770\n",
      "Trainable params: 770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 770\n",
      "Trainable params: 770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.5503\n",
      "Episode 1, Reward -191.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.8837\n",
      "Episode 2, Reward -191.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.3828\n",
      "Episode 3, Reward -191.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.5503\n",
      "Episode 4, Reward -190.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.3891\n",
      "Episode 5, Reward -192.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.8488\n",
      "Episode 6, Reward -191.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 384.0191\n",
      "Episode 7, Reward -176.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 76.8949\n",
      "Episode 8, Reward -192.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 51.8454\n",
      "Episode 9, Reward -192.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 341.0193\n",
      "Episode 10, Reward -192.0\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 232.0786\n",
      "Episode 11, Reward -175.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.4469\n",
      "Episode 12, Reward -177.0\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 15.6854\n",
      "Episode 13, Reward -178.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 117.9612\n",
      "Episode 14, Reward -175.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 136.5556\n",
      "Episode 15, Reward -188.0\n",
      "1/1 [==============================] - 0s 516us/step - loss: 96.9532\n",
      "Episode 16, Reward -191.0\n",
      "1/1 [==============================] - 0s 508us/step - loss: 324.3588\n",
      "Episode 17, Reward -191.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 20.8358\n",
      "Episode 18, Reward -189.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 85.3903\n",
      "Episode 19, Reward -190.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 211.2465\n",
      "Episode 20, Reward -158.0\n",
      "1/1 [==============================] - 0s 559us/step - loss: 51.0893\n",
      "Episode 21, Reward -113.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 121.2508\n",
      "Episode 22, Reward -154.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0701\n",
      "Episode 23, Reward -61.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 918.1337\n",
      "Episode 24, Reward -128.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1065.4076\n",
      "Episode 25, Reward -104.0\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 898.8517\n",
      "Episode 26, Reward -122.0\n",
      "1/1 [==============================] - 0s 528us/step - loss: 8.9839\n",
      "Episode 27, Reward -127.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 21.0875\n",
      "Episode 28, Reward -105.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.4118\n",
      "Episode 29, Reward -81.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 35.2376\n",
      "Episode 30, Reward -110.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.3544\n",
      "Episode 31, Reward -104.0\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 11.3058\n",
      "Episode 32, Reward -113.0\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 19.3105\n",
      "Episode 33, Reward -100.0\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 122.7457\n",
      "Episode 34, Reward -97.0\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.0931\n",
      "Episode 35, Reward -95.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.1635\n",
      "Episode 36, Reward -81.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1427\n",
      "Episode 37, Reward -40.0\n",
      "1/1 [==============================] - 0s 574us/step - loss: 1833.2606\n",
      "Episode 38, Reward -79.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.3219\n",
      "Episode 39, Reward -59.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5585\n",
      "Episode 40, Reward -49.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 217.8551\n",
      "Episode 41, Reward -72.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.5074\n",
      "Episode 42, Reward -51.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.5763\n",
      "Episode 43, Reward -51.0\n",
      "1/1 [==============================] - 0s 509us/step - loss: 26.1901\n",
      "Episode 44, Reward -66.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1870.1979\n",
      "Episode 45, Reward -84.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 13.7687\n",
      "Episode 46, Reward -41.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 21.8553\n",
      "Episode 47, Reward -54.0\n",
      "1/1 [==============================] - 0s 626us/step - loss: 1.9609\n",
      "Episode 48, Reward -25.0\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4197\n",
      "Episode 49, Reward -17.0\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.2719\n",
      "Episode 50, Reward 79.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.3135\n",
      "Episode 51, Reward 19.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 85.4946\n",
      "Episode 52, Reward 27.0\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.0459\n",
      "Episode 53, Reward 1.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9627\n",
      "Episode 54, Reward 84.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.4269\n",
      "Episode 55, Reward 36.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.1890\n",
      "Episode 56, Reward 275.0\n",
      "1/1 [==============================] - 0s 517us/step - loss: 41.4563\n",
      "Episode 57, Reward 209.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 102.5099\n",
      "Episode 58, Reward 108.0\n",
      "1/1 [==============================] - 0s 508us/step - loss: 1.8951\n",
      "Episode 59, Reward 83.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 22.3678\n",
      "Episode 60, Reward 299.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.4286\n",
      "Episode 61, Reward 299.0\n",
      "1/1 [==============================] - 0s 518us/step - loss: 25.2294\n",
      "Episode 62, Reward 299.0\n",
      "1/1 [==============================] - 0s 511us/step - loss: 4.5027\n",
      "Episode 63, Reward -41.0\n",
      "1/1 [==============================] - 0s 510us/step - loss: 3.3849\n",
      "Episode 64, Reward -27.0\n",
      "1/1 [==============================] - 0s 671us/step - loss: 161.4402\n",
      "Episode 65, Reward -44.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3644\n",
      "Episode 66, Reward -50.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.9677\n",
      "Episode 67, Reward -14.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.1797\n",
      "Episode 68, Reward -31.0\n",
      "1/1 [==============================] - 0s 551us/step - loss: 18.4213\n",
      "Episode 69, Reward -39.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 10.0715\n",
      "Episode 70, Reward 150.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 25.7271\n",
      "Episode 71, Reward -21.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9.9405\n",
      "Episode 72, Reward -66.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 36.9261\n",
      "Episode 73, Reward 299.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8.0734\n",
      "Episode 74, Reward 11.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.9742\n",
      "Episode 75, Reward -7.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.1697\n",
      "Episode 76, Reward -43.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.6027\n",
      "Episode 77, Reward 299.0\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.0148\n",
      "Episode 78, Reward 100.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 119.0389\n",
      "Episode 79, Reward 299.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4722\n",
      "Episode 80, Reward 44.0\n",
      "1/1 [==============================] - 0s 514us/step - loss: 5.8572\n",
      "Episode 81, Reward -19.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 12.1104\n",
      "Episode 82, Reward -64.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3677\n",
      "Episode 83, Reward 299.0\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 19.0037\n",
      "Episode 84, Reward 155.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 16.7806\n",
      "Episode 85, Reward 41.0\n",
      "1/1 [==============================] - 0s 677us/step - loss: 7.6417\n",
      "Episode 86, Reward 111.0\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9436\n",
      "Episode 87, Reward 299.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6814\n",
      "Episode 88, Reward 118.0\n",
      "1/1 [==============================] - 0s 562us/step - loss: 12.4315\n",
      "Episode 89, Reward 32.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5058\n",
      "Episode 90, Reward 299.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 10.0593\n",
      "Episode 91, Reward 54.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 67.4372\n",
      "Episode 92, Reward 299.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.1187\n",
      "Episode 93, Reward 94.0\n",
      "1/1 [==============================] - 0s 662us/step - loss: 1.7756\n",
      "Episode 94, Reward -14.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0380\n",
      "Episode 95, Reward 76.0\n",
      "1/1 [==============================] - 0s 510us/step - loss: 0.4981\n",
      "Episode 96, Reward -5.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9569\n",
      "Episode 97, Reward -29.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0840\n",
      "Episode 98, Reward -34.0\n",
      "1/1 [==============================] - 0s 595us/step - loss: 3.3517\n",
      "Episode 99, Reward 2.0\n",
      "1/1 [==============================] - 0s 507us/step - loss: 1.0052\n",
      "Episode 100, Reward 78.0\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 14.1928\n",
      "Episode 101, Reward 102.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0223\n",
      "Episode 102, Reward 223.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.9873\n",
      "Episode 103, Reward 299.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.6898\n",
      "Episode 104, Reward 104.0\n",
      "1/1 [==============================] - 0s 510us/step - loss: 0.6265\n",
      "Episode 105, Reward 32.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 16.7837\n",
      "Episode 106, Reward 40.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1137\n",
      "Episode 107, Reward 81.0\n",
      "1/1 [==============================] - 0s 626us/step - loss: 97.6474\n",
      "Episode 108, Reward 32.0\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8094\n",
      "Episode 109, Reward 49.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.1667\n",
      "Episode 110, Reward 102.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.2782\n",
      "Episode 111, Reward 139.0\n",
      "1/1 [==============================] - 0s 623us/step - loss: 4.4049\n",
      "Episode 112, Reward 123.0\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.4608\n",
      "Episode 113, Reward 71.0\n",
      "1/1 [==============================] - 0s 535us/step - loss: 14.6723\n",
      "Episode 114, Reward 79.0\n",
      "1/1 [==============================] - 0s 515us/step - loss: 8.6404\n",
      "Episode 115, Reward 196.0\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 14.2730\n",
      "Episode 116, Reward 291.0\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.9771\n",
      "Episode 117, Reward 299.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "%matplotlib inline\n",
    "\n",
    "#class agent\n",
    "class DQNAgent():\n",
    "    \n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size \n",
    "        self.action_size = action_size \n",
    "        \n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.01\n",
    "        self.epsilon = 0.1\n",
    "        self.epsilon_decay_factor = 0.999\n",
    "        \n",
    "        #Experience Replay technic คนละ Weight\n",
    "        self.model = self.build_model() #ใช้หา 1.Optimal action ในแต่ละ timestep, 2.ใช้คำนวน Q(s,a) ใน loss function \n",
    "        self.target_model = self.build_model() #Target model ใช้คำนวน (r + gamma * maxQ(s',a)) \n",
    "        \n",
    "        self.update_target_model() \n",
    "        \n",
    "        self.memory = deque(maxlen=100000) #memory for train\n",
    "        self.batch_size = 60  \n",
    "        \n",
    "    def update_target_model(self):  \n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        optimal_action = np.argmax(self.model.predict(state)[0]) \n",
    "        random_action = random.randint(0,self.action_size-1)\n",
    "        action = np.random.choice([optimal_action, random_action], p=[1-self.epsilon, self.epsilon])\n",
    "        return action \n",
    "        \n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(units=self.action_size, activation='linear'))\n",
    "        model.summary()\n",
    "        model.compile(loss='mse',optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def save_sample(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "         \n",
    "    def train_model(self):\n",
    "        if len(self.memory) >= self.batch_size:\n",
    "            mini_batch = random.sample(self.memory, self.batch_size)  # random sample for cut correlation\n",
    "\n",
    "            update_state = np.zeros((self.batch_size, self.state_size))\n",
    "            update_next_state = np.zeros((self.batch_size, self.state_size))\n",
    "            action, reward, done = np.empty([self.batch_size]), np.empty([self.batch_size]), np.empty([self.batch_size])\n",
    "\n",
    "            for i in range(self.batch_size):\n",
    "                update_state[i] = mini_batch[i][0]\n",
    "                action[i] = mini_batch[i][1]\n",
    "                reward[i] = mini_batch[i][2]\n",
    "                update_next_state[i] = mini_batch[i][3]\n",
    "                done[i] = mini_batch[i][4]\n",
    "                \n",
    "            q_next = self.target_model.predict(update_next_state)\n",
    "            q_current = self.model.predict(update_state)\n",
    "            \n",
    "            for i in range(self.batch_size):\n",
    "                if done[i]:\n",
    "                    q_current[i][action.astype(int)[i]] = reward[i]\n",
    "                else:\n",
    "                    q_current[i][action.astype(int)[i]] = reward[i] + self.discount_factor * (np.amax(q_next[i]))\n",
    "\n",
    "            self.model.fit(update_state, q_current, epochs=1, verbose=0)\n",
    "\n",
    "        self.epsilon *= self.epsilon_decay_factor\n",
    "        \n",
    "    def evaluate(self):\n",
    "        n_sample = 1\n",
    "        sample = random.sample(self.memory, n_sample)\n",
    "\n",
    "        state = np.zeros((n_sample, self.state_size))\n",
    "        next_state = np.zeros((n_sample, self.state_size))\n",
    "        action, reward, done = np.empty([n_sample]), np.empty([n_sample]), np.empty([n_sample])\n",
    "\n",
    "        for i in range(n_sample):\n",
    "            state[i] = sample[i][0]\n",
    "            action[i] = sample[i][1]\n",
    "            reward[i] = sample[i][2]\n",
    "            next_state[i] = sample[i][3]\n",
    "            done[i] = sample[i][4]\n",
    "\n",
    "        target = reward + self.discount_factor * np.max(self.target_model.predict(next_state), axis=1) * (np.ones(n_sample) - done)\n",
    "        loss = self.model.evaluate(state, target)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    env = gym.make('CartPole-v1')\n",
    "    agent = DQNAgent(4,2)\n",
    "    number_of_episodes = 400\n",
    "    total_reward = np.zeros(number_of_episodes)\n",
    "    loss = np.zeros(number_of_episodes)\n",
    "    \n",
    "\n",
    "    for i in range(number_of_episodes):\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1,4])\n",
    "           \n",
    "        while True:\n",
    "            #env.render()\n",
    "            \n",
    "            #take action\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1,4])\n",
    "            \n",
    "            if done: \n",
    "                reward = -200\n",
    "            else:\n",
    "                reward = reward\n",
    "                \n",
    "            agent.save_sample(state, action, reward, next_state, done)\n",
    "                \n",
    "            agent.train_model()\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward[i] += reward\n",
    "\n",
    "            if done:\n",
    "                #env.render()\n",
    "                \n",
    "                agent.update_target_model()\n",
    "                \n",
    "                loss[i] = agent.evaluate()\n",
    "    \n",
    "                text_file = open(\"text.txt\", \"a\") \n",
    "                text_file.write('{}\\n{}\\n'.format(total_reward[i], loss[i]))\n",
    "                text_file.close()\n",
    "                \n",
    "                print(\"Episode {}, Reward {}\".format(i+1, total_reward[i]))\n",
    "                \n",
    "                break\n",
    "        \n",
    "        \n",
    "    plt.plot(total_reward)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.title('Performance',fontsize=18)\n",
    "    plt.show()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
